{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "An autoencoder is a type of neural network architecture that learns to encode input data into a lower-dimensional representation (encoding) and then decode it back to reconstruct the original input. It consists of two main components: an encoder and a decoder.\n",
    "\n",
    "- **Encoder**: The encoder component of an autoencoder takes the input data and compresses it into a lower-dimensional representation, also known as the encoding. This encoding typically has fewer dimensions than the original input data, effectively capturing its key features and patterns.\n",
    "\n",
    "- **Decoder**: The decoder component of an autoencoder takes the encoded representation (output of the encoder) and reconstructs the original input data from it. The decoder tries to produce an output that closely matches the input data, effectively learning to reconstruct the data from its compressed representation.\n",
    "\n",
    "Autoencoders are trained using unsupervised learning, where the objective is to minimize the reconstruction error between the input data and the reconstructed output. By learning to reconstruct the input data, autoencoders effectively learn a compressed representation of the data while capturing its essential features.\n",
    "\n",
    "In the context of anomaly detection, autoencoders can be used to detect anomalies by leveraging the reconstruction error. During training, the autoencoder learns to reconstruct normal data with low error. However, anomalies, which deviate significantly from the normal data distribution, are likely to result in higher reconstruction errors.\n",
    "\n",
    "After training, the autoencoder can be used to reconstruct new data samples. Anomalies are identified by comparing the reconstruction error of each data sample to a predefined threshold. Data samples with reconstruction errors above the threshold are considered anomalies.\n",
    "\n",
    "In summary, autoencoders can be utilized for anomaly detection by learning to reconstruct normal data and identifying deviations from the normal data distribution based on reconstruction errors. They are particularly effective for detecting anomalies in data with complex patterns and structures.\n",
    "\n",
    "In this exercise we will train an autoencoder to detect anomalies in the sinusoid signal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "## Exercise 1\n",
    "\n",
    "Generate the signal, add noise and normalize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Generate synthetic sinusoidal signal\n",
    "t = np.linspace(0, 100, 1000)\n",
    "signal = np.sin(t) \n",
    "\n",
    "# Add anomalies to the signal\n",
    "anomalies = np.random.normal(loc=0, scale=0.5, size=1000)  # Generate noise\n",
    "\n",
    "signal_with_anomalies = np.append(signal[0:800], anomalies[0:200])  # Signal with anomalies\n",
    "\n",
    "# Normalize the signals\n",
    "max_val = np.max(np.append(anomalies, signal))\n",
    "signal_normalized = signal / max_val\n",
    "signal_with_anomalies_normalized = signal_with_anomalies / max_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "Generate sequences of the signal with a certain timewindow.\n",
    "The window should loop over a list and you should collect all these sublists in another list.\n",
    "\n",
    "Tip: what do you think a good timestep should be for the network to be able to learn the signal? \n",
    "Tip: think about the periodicity of the signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generated training sequences for use in the model.\n",
    "TIME_STEPS = 60\n",
    "def create_sequences(values, time_steps):\n",
    "    output = []\n",
    "    for i in range(len(values) - time_steps + 1):\n",
    "        output.append(values[i : (i + time_steps)])\n",
    "    return np.stack(output) \n",
    "\n",
    "signal_normalized = create_sequences(signal_normalized, TIME_STEPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "Create a train and testset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "train_data = signal_normalized[:800]\n",
    "val_data = signal_normalized[800:]\n",
    "random.shuffle(train_data)\n",
    "random.shuffle(val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "\n",
    "Create the auto encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the autoencoder model\n",
    "input_layer = Input(shape=(TIME_STEPS,))\n",
    "encoded = Dense(64, activation='relu')(input_layer)\n",
    "encoded = Dense(32, activation='relu')(encoded)\n",
    "encoded = Dense(16, activation='relu')(encoded)\n",
    "decoded = Dense(32, activation='relu')(encoded)\n",
    "decoded = Dense(64, activation='relu')(decoded)\n",
    "decoded = Dense(TIME_STEPS, activation='sigmoid')(decoded)\n",
    "\n",
    "autoencoder = Model(input_layer, decoded)\n",
    "\n",
    "# Compile the autoencoder\n",
    "autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the autoencoder\n",
    "autoencoder.fit(train_data, train_data, epochs=100, batch_size=32, shuffle=True, validation_data=(val_data, val_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5\n",
    "\n",
    "Use the network to predict the signal with anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add anomalies to the signal\n",
    "anomalies = np.random.normal(loc=0, scale=0.5, size=1000)  # Generate noise\n",
    "signal_with_anomalies = np.append(signal[0:800], anomalies[0:200])  # Signal with anomalies\n",
    "signal_with_anomalies_normalized = signal_with_anomalies / max_val\n",
    "\n",
    "test_data = create_sequences(signal_with_anomalies_normalized, TIME_STEPS)\n",
    "\n",
    "# Predict on the test data\n",
    "predictions = autoencoder.predict(test_data)\n",
    "\n",
    "# Calculate the reconstruction error\n",
    "reconstruction_error = np.mean(np.square(predictions - test_data), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6\n",
    "\n",
    "Plot the original signal, the signal with anomalies and the predicted signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the original signal, signal with anomalies, and reconstruction error\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.title('Original Signal')\n",
    "plt.plot(signal)\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.title('Signal with Anomalies')\n",
    "plt.plot(signal_with_anomalies)\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.title('Reconstruction Error')\n",
    "plt.plot(reconstruction_error, color='r')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 7 \n",
    "\n",
    "How would you now detect the anomaly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect anomalies based on reconstruction error\n",
    "threshold = np.mean(reconstruction_error) + 2 * np.std(reconstruction_error)  # Adjust the threshold as needed\n",
    "anomaly_indices = np.where(reconstruction_error > threshold)[0]\n",
    "\n",
    "print(f\"Detected anomalies at indices: {anomaly_indices}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "interay-ofPTlAwe-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
