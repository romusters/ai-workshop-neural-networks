{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "TensorFlow Lite (TFLite) is a lightweight version of TensorFlow, an open-source machine learning framework developed by Google. TFLite is designed specifically for running machine learning models on edge devices with limited computational resources, such as mobile devices, IoT (Internet of Things) devices, and embedded systems.\n",
    "\n",
    "TFLite allows developers to deploy trained TensorFlow models on these edge devices, enabling on-device inference without requiring a constant internet connection or relying on cloud services. This is particularly useful for applications where low latency, privacy, and offline operation are important considerations.\n",
    "\n",
    "Key features of TensorFlow Lite include:\n",
    "\n",
    "1. **Model Optimization**: TFLite provides tools and techniques for optimizing TensorFlow models for deployment on edge devices, including quantization, model pruning, and post-training quantization.\n",
    "\n",
    "2. **Interpreter**: TFLite includes an interpreter that allows models to be executed efficiently on various hardware platforms, including CPUs, GPUs, and specialized accelerators like Neural Processing Units (NPUs) and Graphics Processing Units (GPUs).\n",
    "\n",
    "3. **Flexibility**: TFLite supports a wide range of TensorFlow model architectures, including convolutional neural networks (CNNs), recurrent neural networks (RNNs), and custom models built using TensorFlow's high-level APIs.\n",
    "\n",
    "4. **Portability**: TFLite models can be easily ported across different platforms and operating systems, including Android, iOS, Linux, and microcontroller-based systems.\n",
    "\n",
    "5. **Integration**: TFLite seamlessly integrates with popular development environments and frameworks, such as Android Studio, TensorFlow.js, and TensorFlow Lite for Microcontrollers.\n",
    "\n",
    "Overall, TensorFlow Lite provides a streamlined solution for deploying machine learning models on edge devices, enabling efficient inference and real-time decision-making in resource-constrained environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example\n",
    "\n",
    "An example on how to use TFLite using a simple Neural Network is given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Step 1: Create and train a simple neural network\n",
    "# Generate some dummy data for demonstration\n",
    "X_train = np.random.rand(100, 10)\n",
    "y_train = np.random.randint(0, 2, size=(100,))\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=(10,)),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
    "\n",
    "# Step 2: Save the TensorFlow model\n",
    "model.export(\"../models/model.pb\")\n",
    "\n",
    "# Step 3: Convert the TensorFlow model to TFLite\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(\"../models/model.pb\")\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the TFLite model to a file\n",
    "with open('../models/model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "print(\"TFLite model saved successfully!\")\n",
    "# https://github.com/tensorflow/tensorflow/issues/63987"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "## Exercise 1: Reduced model size\n",
    "\n",
    "Check the size of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "original_model_size = os.path.getsize('../models/model.pb')\n",
    "\n",
    "# Get the size of the converted TFLite model file\n",
    "tflite_model_size = os.path.getsize('../models/model.tflite')\n",
    "\n",
    "# Print the sizes of the original model and the converted TFLite model\n",
    "print(f\"Original TensorFlow model size: {original_model_size} bytes\")\n",
    "print(f\"Converted TFLite model size: {tflite_model_size} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of the lite model is actually larger. This is because we've taken a small example neural network. When using a larger network, the difference will be more obvious."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "Now use your saved RNN prediction model and convert it.\n",
    "\n",
    "Tip: Use `model.export(filepath)` if you want to export a SavedModel for use with TFLite/TFServing/etc. Received: filepath=simple_nn_model.pb.\n",
    "\n",
    "If you want to optimize the model further, check out: https://www.tensorflow.org/lite/performance/model_optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(\"../models/model.pb\")\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the TFLite model to a file\n",
    "with open('../models/model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_model_size = os.path.getsize('../models/model.pb')\n",
    "\n",
    "# Get the size of the converted TFLite model file\n",
    "tflite_model_size = os.path.getsize('../models/model.tflite')\n",
    "\n",
    "# Print the sizes of the original model and the converted TFLite model\n",
    "print(f\"Original TensorFlow model size: {original_model_size} bytes\")\n",
    "print(f\"Converted TFLite model size: {tflite_model_size} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus exercise\n",
    "\n",
    "Try to run it on a Raspberry Pi!\n",
    "\n",
    "For more information: https://www.tensorflow.org/lite/guide/python"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "interay-ofPTlAwe-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
